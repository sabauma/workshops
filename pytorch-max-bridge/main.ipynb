{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4186aa4e-4e72-4b9a-bfa8-b36ec14da998",
   "metadata": {},
   "source": [
    "# Bridging PyTorch with MAX🧑‍🚀 and Mojo🔥: \n",
    "\n",
    "Accompanying code for [workshops/pytorch-max-bridge](https://github.com/modular/workshops/tree/main/pytorch-max-bridge).\n",
    "\n",
    "**We will learn how to gradually replace any parts of a PyTorch model with MAX and Mojo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4703559-869f-4ac8-8a66-7db6d4fdf1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f4a3264-7a2d-4780-8c51-42dcd1815034",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 1024)\n",
       "    (wpe): Embedding(1024, 1024)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-23): 24 x GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=3072, nx=1024)\n",
       "          (c_proj): Conv1D(nf=1024, nx=1024)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=4096, nx=1024)\n",
       "          (c_proj): Conv1D(nf=1024, nx=4096)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# no need to copy\n",
    "model.to(device)\n",
    "model.eval()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b35405c-5cf8-413c-a1b8-30a2f4816fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "class DialogueBot:\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.chat_history_ids = None\n",
    "\n",
    "    def generate_response(self, user_input):\n",
    "        with torch.no_grad():\n",
    "            set_seed(42)\n",
    "            new_user_input_ids = self.tokenizer.encode(\n",
    "                user_input + self.tokenizer.eos_token,\n",
    "                return_tensors='pt'\n",
    "            ).to(device)\n",
    "\n",
    "            if self.chat_history_ids is not None:\n",
    "                bot_input_ids = torch.cat([self.chat_history_ids, new_user_input_ids], dim=-1).to(device)\n",
    "            else:\n",
    "                bot_input_ids = new_user_input_ids\n",
    "\n",
    "            # Create attention mask (1 for real tokens, 0 for padding)\n",
    "            # Since we're not using padding here, all tokens are real\n",
    "            attention_mask = torch.ones_like(bot_input_ids).to(device)\n",
    "\n",
    "            self.chat_history_ids = self.model.generate(\n",
    "                bot_input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_length=1000,\n",
    "                pad_token_id=self.tokenizer.eos_token_id,\n",
    "                repetition_penalty=1.1,\n",
    "                no_repeat_ngram_size=2,\n",
    "            )\n",
    "\n",
    "            response = self.tokenizer.decode(\n",
    "                self.chat_history_ids[:, bot_input_ids.shape[-1]:][0],\n",
    "                skip_special_tokens=True\n",
    "            )\n",
    "            return response\n",
    "\n",
    "    def reset_conversation(self):\n",
    "        self.chat_history_ids = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eda46ddd-d27b-4afd-b0ff-2d2a6706dfd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== STAGE 1: Original PyTorch Implementation ===\n",
      "Original PyTorch Results:\n",
      "Turn 1\n",
      "User: Hello, how are you?\n",
      "Bot: I'm good, thanks. How about you?\n",
      "================================================================================\n",
      "Turn 2\n",
      "User: What's your favorite programming language?\n",
      "Bot: Java, Python, C, and C.\n",
      "================================================================================\n",
      "Turn 3\n",
      "User: Tell me about artificial intelligence\n",
      "Bot: It's a thing. It's called AI. I don't know what it is, but it's something. And it can be used to make things. Like computers. Or robots. Whatever. You want it to be. :D\n",
      "================================================================================\n",
      "Turn 4\n",
      "User: What's the meaning of life?\n",
      "Bot: Life is a computer program that makes things\n",
      "================================================================================\n",
      "Turn 5\n",
      "User: Tell me a joke\n",
      "Bot: A joke!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "test_inputs = [\n",
    "    \"Hello, how are you?\",\n",
    "    \"What's your favorite programming language?\",\n",
    "    \"Tell me about artificial intelligence\",\n",
    "    \"What's the meaning of life?\",\n",
    "    \"Tell me a joke\",\n",
    "]\n",
    "\n",
    "print(\"=== STAGE 1: Original PyTorch Implementation ===\")\n",
    "bot_original = DialogueBot(model, tokenizer)\n",
    "\n",
    "print(\"Original PyTorch Results:\")\n",
    "original_responses = []\n",
    "for i, user_input in enumerate(test_inputs, 1):\n",
    "    response = bot_original.generate_response(user_input)\n",
    "    original_responses.append(response)\n",
    "    print(f\"Turn {i}\")\n",
    "    print(f\"User: {user_input}\")\n",
    "    print(f\"Bot: {response}\")\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8316dd1e-e994-4b6f-a494-912a0857dc8b",
   "metadata": {},
   "source": [
    "## Custom Op with [max.torch.graph_op](https://docs.modular.com/max/api/python/torch/#max.torch.graph_op)\n",
    "\n",
    "* This is a powerful approach that leverages writting composable custom op using MAX Python Graph [max.graph.ops](https://docs.modular.com/max/api/python/graph/ops/) APIs.\n",
    "* MAX Graph Compiler performs various optimizations such as kernel fusion.\n",
    "* `torch.compile` helps with memory planning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7fce5d3-a615-4141-aae8-370e4fb584a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import max.torch\n",
    "from max.graph import TensorValue, ops\n",
    "\n",
    "@max.torch.graph_op\n",
    "def max_layer_norm(x: TensorValue, weight: TensorValue, bias: TensorValue) -> TensorValue:\n",
    "    return ops.layer_norm(x, weight, bias, epsilon=1e-5)\n",
    "\n",
    "@torch.compile  \n",
    "def custom_layer_norm(x: torch.Tensor, weight: torch.Tensor, bias: torch.Tensor) -> torch.Tensor:\n",
    "    # same as torch.empty with correct dtype and device mapping\n",
    "    output = x.new_empty(x.shape)\n",
    "    # NOTE: `output` is added as an argument to enable highly efficient Destination-Passing Style\n",
    "    max_layer_norm(output, x, weight, bias)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "031a874c-757e-4849-92dc-b91bdf9afe89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick test\n",
    "import torch.nn as nn\n",
    "\n",
    "batch_size, seq_len, hidden_size = 1, 8, 32\n",
    "test_input = torch.randn(batch_size, seq_len, hidden_size, device=device)\n",
    "test_weight = torch.randn(hidden_size, device=device)\n",
    "test_bias = torch.randn(hidden_size, device=device)\n",
    "\n",
    "pret = nn.functional.layer_norm(test_input, test_weight.shape, test_weight, test_bias)\n",
    "mret = custom_layer_norm(test_input, test_weight, test_bias)\n",
    "\n",
    "assert torch.allclose(pret, mret, 1e-4), f\"didn't match {pret[0, :3, :3]}, {mret[0, :3, :3]}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955c3014-a1cf-4bef-a1ee-27a6b5c499c2",
   "metadata": {},
   "source": [
    "## Perform \"Model Surgery\"\n",
    "\n",
    "Iteratively replacing the `LayerNorm` with our custom op and test the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef019db0-4312-4a0e-a133-9e4b21a7b7ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== STAGE 2: Replacing LayerNorm with MAX ===\n",
      "Found LayerNorm: transformer.h.0.ln_1, weight shape: torch.Size([1024]), bias: True\n",
      "Replaced transformer.h.0.ln_1 with MAX implementation\n",
      "Found LayerNorm: transformer.h.0.ln_2, weight shape: torch.Size([1024]), bias: True\n",
      "Replaced transformer.h.0.ln_2 with MAX implementation\n",
      "Found LayerNorm: transformer.h.1.ln_1, weight shape: torch.Size([1024]), bias: True\n",
      "Replaced transformer.h.1.ln_1 with MAX implementation\n",
      "Found LayerNorm: transformer.h.1.ln_2, weight shape: torch.Size([1024]), bias: True\n",
      "Replaced transformer.h.1.ln_2 with MAX implementation\n",
      "Found LayerNorm: transformer.h.2.ln_1, weight shape: torch.Size([1024]), bias: True\n",
      "Replaced transformer.h.2.ln_1 with MAX implementation\n",
      "Found LayerNorm: transformer.h.2.ln_2, weight shape: torch.Size([1024]), bias: True\n",
      "Replaced transformer.h.2.ln_2 with MAX implementation\n",
      "Found LayerNorm: transformer.h.3.ln_1, weight shape: torch.Size([1024]), bias: True\n",
      "Replaced transformer.h.3.ln_1 with MAX implementation\n",
      "Found LayerNorm: transformer.h.3.ln_2, weight shape: torch.Size([1024]), bias: True\n",
      "Replaced transformer.h.3.ln_2 with MAX implementation\n",
      "Found LayerNorm: transformer.h.4.ln_1, weight shape: torch.Size([1024]), bias: True\n",
      "Replaced transformer.h.4.ln_1 with MAX implementation\n",
      "Found LayerNorm: transformer.h.4.ln_2, weight shape: torch.Size([1024]), bias: True\n",
      "Replaced transformer.h.4.ln_2 with MAX implementation\n",
      "Found LayerNorm: transformer.h.5.ln_1, weight shape: torch.Size([1024]), bias: True\n",
      "Replaced transformer.h.5.ln_1 with MAX implementation\n",
      "Found LayerNorm: transformer.h.5.ln_2, weight shape: torch.Size([1024]), bias: True\n",
      "Replaced transformer.h.5.ln_2 with MAX implementation\n",
      "Found LayerNorm: transformer.h.6.ln_1, weight shape: torch.Size([1024]), bias: True\n",
      "Replaced transformer.h.6.ln_1 with MAX implementation\n",
      "Found LayerNorm: transformer.h.6.ln_2, weight shape: torch.Size([1024]), bias: True\n",
      "Replaced transformer.h.6.ln_2 with MAX implementation\n",
      "Found LayerNorm: transformer.h.7.ln_1, weight shape: torch.Size([1024]), bias: True\n",
      "Replaced transformer.h.7.ln_1 with MAX implementation\n",
      "Found LayerNorm: transformer.h.7.ln_2, weight shape: torch.Size([1024]), bias: True\n",
      "Replaced transformer.h.7.ln_2 with MAX implementation\n",
      "Found LayerNorm: transformer.h.8.ln_1, weight shape: torch.Size([1024]), bias: True\n",
      "Replaced transformer.h.8.ln_1 with MAX implementation\n",
      "Found LayerNorm: transformer.h.8.ln_2, weight shape: torch.Size([1024]), bias: True\n",
      "Replaced transformer.h.8.ln_2 with MAX implementation\n",
      "Found LayerNorm: transformer.h.9.ln_1, weight shape: torch.Size([1024]), bias: True\n",
      "Replaced transformer.h.9.ln_1 with MAX implementation\n",
      "Found LayerNorm: transformer.h.9.ln_2, weight shape: torch.Size([1024]), bias: True\n",
      "Replaced transformer.h.9.ln_2 with MAX implementation\n",
      "Found LayerNorm: transformer.h.10.ln_1, weight shape: torch.Size([1024]), bias: True\n",
      "Replaced transformer.h.10.ln_1 with MAX implementation\n",
      "Found LayerNorm: transformer.h.10.ln_2, weight shape: torch.Size([1024]), bias: True\n",
      "Replaced transformer.h.10.ln_2 with MAX implementation\n",
      "Found LayerNorm: transformer.h.11.ln_1, weight shape: torch.Size([1024]), bias: True\n",
      "Replaced transformer.h.11.ln_1 with MAX implementation\n",
      "Found LayerNorm: transformer.h.11.ln_2, weight shape: torch.Size([1024]), bias: True\n",
      "Replaced transformer.h.11.ln_2 with MAX implementation\n",
      "Found LayerNorm: transformer.h.12.ln_1, weight shape: torch.Size([1024]), bias: True\n",
      "Replaced transformer.h.12.ln_1 with MAX implementation\n",
      "Found LayerNorm: transformer.h.12.ln_2, weight shape: torch.Size([1024]), bias: True\n",
      "Replaced transformer.h.12.ln_2 with MAX implementation\n",
      "Found LayerNorm: transformer.h.13.ln_1, weight shape: torch.Size([1024]), bias: True\n",
      "Replaced transformer.h.13.ln_1 with MAX implementation\n",
      "Found LayerNorm: transformer.h.13.ln_2, weight shape: torch.Size([1024]), bias: True\n",
      "Replaced transformer.h.13.ln_2 with MAX implementation\n",
      "Found LayerNorm: transformer.h.14.ln_1, weight shape: torch.Size([1024]), bias: True\n",
      "Replaced transformer.h.14.ln_1 with MAX implementation\n",
      "Found LayerNorm: transformer.h.14.ln_2, weight shape: torch.Size([1024]), bias: True\n",
      "Replaced transformer.h.14.ln_2 with MAX implementation\n",
      "Found LayerNorm: transformer.h.15.ln_1, weight shape: torch.Size([1024]), bias: True\n",
      "Replaced transformer.h.15.ln_1 with MAX implementation\n",
      "Found LayerNorm: transformer.h.15.ln_2, weight shape: torch.Size([1024]), bias: True\n",
      "Replaced transformer.h.15.ln_2 with MAX implementation\n",
      "Found LayerNorm: transformer.h.16.ln_1, weight shape: torch.Size([1024]), bias: True\n",
      "Replaced transformer.h.16.ln_1 with MAX implementation\n",
      "Found LayerNorm: transformer.h.16.ln_2, weight shape: torch.Size([1024]), bias: True\n",
      "Replaced transformer.h.16.ln_2 with MAX implementation\n",
      "Found LayerNorm: transformer.h.17.ln_1, weight shape: torch.Size([1024]), bias: True\n",
      "Replaced transformer.h.17.ln_1 with MAX implementation\n",
      "Found LayerNorm: transformer.h.17.ln_2, weight shape: torch.Size([1024]), bias: True\n",
      "Replaced transformer.h.17.ln_2 with MAX implementation\n",
      "Found LayerNorm: transformer.h.18.ln_1, weight shape: torch.Size([1024]), bias: True\n",
      "Replaced transformer.h.18.ln_1 with MAX implementation\n",
      "Found LayerNorm: transformer.h.18.ln_2, weight shape: torch.Size([1024]), bias: True\n",
      "Replaced transformer.h.18.ln_2 with MAX implementation\n",
      "Found LayerNorm: transformer.h.19.ln_1, weight shape: torch.Size([1024]), bias: True\n",
      "Replaced transformer.h.19.ln_1 with MAX implementation\n",
      "Found LayerNorm: transformer.h.19.ln_2, weight shape: torch.Size([1024]), bias: True\n",
      "Replaced transformer.h.19.ln_2 with MAX implementation\n",
      "Found LayerNorm: transformer.h.20.ln_1, weight shape: torch.Size([1024]), bias: True\n",
      "Replaced transformer.h.20.ln_1 with MAX implementation\n",
      "Found LayerNorm: transformer.h.20.ln_2, weight shape: torch.Size([1024]), bias: True\n",
      "Replaced transformer.h.20.ln_2 with MAX implementation\n",
      "Found LayerNorm: transformer.h.21.ln_1, weight shape: torch.Size([1024]), bias: True\n",
      "Replaced transformer.h.21.ln_1 with MAX implementation\n",
      "Found LayerNorm: transformer.h.21.ln_2, weight shape: torch.Size([1024]), bias: True\n",
      "Replaced transformer.h.21.ln_2 with MAX implementation\n",
      "Found LayerNorm: transformer.h.22.ln_1, weight shape: torch.Size([1024]), bias: True\n",
      "Replaced transformer.h.22.ln_1 with MAX implementation\n",
      "Found LayerNorm: transformer.h.22.ln_2, weight shape: torch.Size([1024]), bias: True\n",
      "Replaced transformer.h.22.ln_2 with MAX implementation\n",
      "Found LayerNorm: transformer.h.23.ln_1, weight shape: torch.Size([1024]), bias: True\n",
      "Replaced transformer.h.23.ln_1 with MAX implementation\n",
      "Found LayerNorm: transformer.h.23.ln_2, weight shape: torch.Size([1024]), bias: True\n",
      "Replaced transformer.h.23.ln_2 with MAX implementation\n",
      "Found LayerNorm: transformer.ln_f, weight shape: torch.Size([1024]), bias: True\n",
      "Replaced transformer.ln_f with MAX implementation\n",
      "Total replaced: 49 LayerNorm operations\n"
     ]
    }
   ],
   "source": [
    "def replace_layer_norm_with_max(model):\n",
    "    \"\"\"Replace LayerNorm with MAX implementation for inference\"\"\"\n",
    "    replaced_count = 0\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.LayerNorm):\n",
    "            print(f\"Found LayerNorm: {name}, weight shape: {module.weight.shape}, bias: {module.bias is not None}\")\n",
    "\n",
    "            if module.weight.shape[0] == 1024:  # DialoGPT-medium hidden dimension\n",
    "                original_forward = module.forward\n",
    "\n",
    "                def max_forward(x, name=name, module=module):\n",
    "                    try:\n",
    "                        weight_detached = module.weight.detach()\n",
    "                        if module.bias is not None:\n",
    "                            bias_detached = module.bias.detach()\n",
    "                            return custom_layer_norm(x, weight_detached, bias_detached)\n",
    "                        else:\n",
    "                            bias_tensor = torch.zeros_like(weight_detached)\n",
    "                            return custom_layer_norm(x, weight_detached, bias_tensor)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error in MAX LayerNorm for {name}: {e}\")\n",
    "                        return original_forward(x)\n",
    "\n",
    "                module.forward = max_forward\n",
    "                replaced_count += 1\n",
    "                print(f\"Replaced {name} with MAX implementation\")\n",
    "            else:\n",
    "                print(f\"Skipping {name} - unexpected shape {module.weight.shape}\")\n",
    "\n",
    "    print(f\"Total replaced: {replaced_count} LayerNorm operations\")\n",
    "    model.to(device)\n",
    "    return replaced_count\n",
    "\n",
    "print(\"=== STAGE 2: Replacing LayerNorm with MAX ===\")\n",
    "replaced_count = replace_layer_norm_with_max(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd249914-102c-4ff9-b2b9-beff557d047f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAX LayerNorm Results:\n",
      "Turn 1\n",
      "User: Hello, how are you?\n",
      "Bot: I'm good, thanks. How about you?\n",
      "================================================================================\n",
      "Turn 2\n",
      "User: What's your favorite programming language?\n",
      "Bot: Java, Python, C, and C.\n",
      "================================================================================\n",
      "Turn 3\n",
      "User: Tell me about artificial intelligence\n",
      "Bot: It's a thing. It's called AI. I don't know what it is, but it's something. And it can be used to make things. Like computers. Or robots. Whatever. You want it to be. :D\n",
      "================================================================================\n",
      "Turn 4\n",
      "User: What's the meaning of life?\n",
      "Bot: Life is a computer program that makes things\n",
      "================================================================================\n",
      "Turn 5\n",
      "User: Tell me a joke\n",
      "Bot: A joke!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "bot_max_layernorm = DialogueBot(model, tokenizer)\n",
    "\n",
    "print(\"MAX LayerNorm Results:\")\n",
    "max_layernorm_responses = []\n",
    "for i, user_input in enumerate(test_inputs, 1):\n",
    "    response = bot_max_layernorm.generate_response(user_input)\n",
    "    max_layernorm_responses.append(response)\n",
    "    print(f\"Turn {i}\")\n",
    "    print(f\"User: {user_input}\")\n",
    "    print(f\"Bot: {response}\")\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fab188d-ee1c-451d-954b-0cb874f6cb34",
   "metadata": {},
   "source": [
    "## Load Mojo Custom op with [max.torch.CustomOpLibrary](https://docs.modular.com/max/api/python/torch/#max.torch.CustomOpLibrary)\n",
    "\n",
    "Another approach is to \n",
    "\n",
    "1. Write a GPU kernel in Mojo.\n",
    "2. Package it [mojo package](https://docs.modular.com/mojo/cli/package/).\n",
    "3. Use `max.torch.CustomOpLibrary` to load the package in our python code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aeea8423-d7ef-4eb8-837c-1af6792b3fa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 1024)\n",
       "    (wpe): Embedding(1024, 1024)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-23): 24 x GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=3072, nx=1024)\n",
       "          (c_proj): Conv1D(nf=1024, nx=1024)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=4096, nx=1024)\n",
       "          (c_proj): Conv1D(nf=1024, nx=4096)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd41767e-231b-4ce1-b6f8-909583f47acb",
   "metadata": {},
   "source": [
    "## Mojo NewGELU Kernel\n",
    "\n",
    "Next, we write a simple CPU/GPU for the `NewGELU` activation function and use the jupyter magic `%%mojo` to make a mojo package (`.mojopkg`) for loading later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5cb86edc-f4e8-494f-9034-0fa79eef976d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import max.support.notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5f8147b-0e19-4213-aa20-53d89f297ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, world!\n",
      "[ Hello, jupyter! ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%mojo\n",
    "\n",
    "@fieldwise_init\n",
    "struct Foo(Writable):\n",
    "    var s: String\n",
    "\n",
    "    fn write_to[W: Writer](self, mut writer: W):\n",
    "        writer.write(\"[ \", self.s, \" ]\")\n",
    "\n",
    "def main():\n",
    "    print(\"Hello, world!\")\n",
    "    foo = Foo(\"Hello, jupyter!\")\n",
    "    print(foo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "38352e40-5249-4da3-aaa0-cb817e5b5bfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%mojo package -o ops.mojopkg\n",
    "\n",
    "import math\n",
    "import compiler\n",
    "from algorithm import parallelize, vectorize\n",
    "from sys import simdwidthof\n",
    "from layout import LayoutTensor, Layout, UNKNOWN_VALUE\n",
    "from gpu import thread_idx, block_idx, block_dim\n",
    "from gpu.host import DeviceContext, DeviceBuffer\n",
    "from runtime.asyncrt import DeviceContextPtr\n",
    "from tensor import InputTensor, OutputTensor\n",
    "from math import ceildiv\n",
    "\n",
    "alias dtype = DType.float32\n",
    "alias BLOCK_SIZE = 256\n",
    "\n",
    "# Core NewGELU computation - can be used in both CPU and GPU contexts\n",
    "@always_inline\n",
    "fn new_gelu_computation[dtype: DType](x: Scalar[dtype]) -> Scalar[dtype]:\n",
    "    \"\"\"\n",
    "    Core NewGELU computation for a single scalar value.\n",
    "\n",
    "    NewGELU(x) = 0.5 * x * (1 + tanh(sqrt(2/pi) * (x + 0.044715 * x^3)))\n",
    "    \"\"\"\n",
    "    alias SQRT_2_OVER_PI = Scalar[dtype](0.7978845608028654)  # sqrt(2/pi) for float32\n",
    "    alias GELU_COEFF = Scalar[dtype](0.044715)\n",
    "\n",
    "    return 0.5 * x * (1.0 + math.tanh(SQRT_2_OVER_PI * (x + GELU_COEFF * x * x * x)))\n",
    "\n",
    "# Simple GPU kernel for demonstration\n",
    "fn new_gelu_gpu_kernel[dtype: DType](\n",
    "    output_ptr: UnsafePointer[Scalar[dtype]],\n",
    "    input_ptr: UnsafePointer[Scalar[dtype]],\n",
    "    num_elements: Int,\n",
    "):\n",
    "    idx = block_idx.x * block_dim.x + thread_idx.x\n",
    "\n",
    "    if idx < num_elements:\n",
    "        x_val = input_ptr[idx]\n",
    "        result_val = new_gelu_computation[dtype](x_val)\n",
    "        output_ptr[idx] = result_val\n",
    "\n",
    "# MAX custom operation using @compiler.register\n",
    "@compiler.register(\"new_gelu\")\n",
    "struct NewGELU:\n",
    "    @staticmethod\n",
    "    fn execute[target: StaticString](\n",
    "        # Outputs\n",
    "        result: OutputTensor[dtype=DType.float32, rank=3], # (batch_size, seq_len, hidden_size)\n",
    "        # Inputs\n",
    "        x: InputTensor[dtype=DType.float32, rank=3],\n",
    "        # Context\n",
    "        ctx: DeviceContextPtr,\n",
    "    ) raises:\n",
    "        batch_size = x.dim_size(0)\n",
    "        seq_len = x.dim_size(1)\n",
    "        hidden_size = x.dim_size(2)\n",
    "\n",
    "        @parameter\n",
    "        if target == \"cpu\":\n",
    "            # Apply NewGELU element-wise across all dimensions\n",
    "            for b in range(batch_size):\n",
    "                for s in range(seq_len):\n",
    "                    for h in range(hidden_size):\n",
    "                        var x_val = x[b, s, h]\n",
    "                        var result_val = new_gelu_computation[dtype](x_val)\n",
    "                        result[b, s, h] = result_val\n",
    "\n",
    "        elif target == \"gpu\":\n",
    "            gpu_ctx = ctx.get_device_context()\n",
    "            num_elements = batch_size * seq_len * hidden_size\n",
    "\n",
    "            # Calculate grid and block dimensions\n",
    "            grid_size = ceildiv(num_elements, BLOCK_SIZE)\n",
    "            block_size = BLOCK_SIZE\n",
    "\n",
    "            # Get tensor data and convert to layout tensors\n",
    "            x_tensor = x.to_layout_tensor()\n",
    "            output_tensor = result.to_layout_tensor()\n",
    "            input_ptr = x_tensor.ptr\n",
    "            output_ptr = output_tensor.ptr\n",
    "\n",
    "            # Launch GPU kernel\n",
    "            gpu_ctx.enqueue_function[new_gelu_gpu_kernel[dtype]](\n",
    "                output_ptr,\n",
    "                input_ptr,\n",
    "                num_elements,\n",
    "                grid_dim=grid_size,\n",
    "                block_dim=block_size,\n",
    "            )\n",
    "        else:\n",
    "            raise Error(\"Unsupported target: \" + target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de18411f-37fd-45aa-a96a-cce53d8d7357",
   "metadata": {},
   "source": [
    "## Load the Custom Mojo Kernel\n",
    "\n",
    "* Load our packaged Mojo kernel using [max.torch.CustomOpLibrary](https://docs.modular.com/max/api/python/torch/#max.torch.CustomOpLibrary).\n",
    "* Perform another surgery to replace the activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82ee87ce-641b-4dd9-af0a-9d8c83602bf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded new_gelu operation\n",
      "  Replacing NewGELUActivation in act\n",
      "  Replacing NewGELUActivation in act\n",
      "  Replacing NewGELUActivation in act\n",
      "  Replacing NewGELUActivation in act\n",
      "  Replacing NewGELUActivation in act\n",
      "  Replacing NewGELUActivation in act\n",
      "  Replacing NewGELUActivation in act\n",
      "  Replacing NewGELUActivation in act\n",
      "  Replacing NewGELUActivation in act\n",
      "  Replacing NewGELUActivation in act\n",
      "  Replacing NewGELUActivation in act\n",
      "  Replacing NewGELUActivation in act\n",
      "  Replacing NewGELUActivation in act\n",
      "  Replacing NewGELUActivation in act\n",
      "  Replacing NewGELUActivation in act\n",
      "  Replacing NewGELUActivation in act\n",
      "  Replacing NewGELUActivation in act\n",
      "  Replacing NewGELUActivation in act\n",
      "  Replacing NewGELUActivation in act\n",
      "  Replacing NewGELUActivation in act\n",
      "  Replacing NewGELUActivation in act\n",
      "  Replacing NewGELUActivation in act\n",
      "  Replacing NewGELUActivation in act\n",
      "  Replacing NewGELUActivation in act\n",
      "Successfully replaced 24 NewGELUActivation operations with MAX\n"
     ]
    }
   ],
   "source": [
    "# Load our custom NewGELU operation from ops package\n",
    "from max.torch import CustomOpLibrary\n",
    "from pathlib import Path\n",
    "\n",
    "try:\n",
    "    op_library = CustomOpLibrary(Path(\"ops.mojopkg\"))\n",
    "    new_gelu_op = op_library.new_gelu\n",
    "    print(\"Successfully loaded new_gelu operation\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading custom operations: {e}\")\n",
    "    print(\"Make sure ops.mojopkg is properly created and contains new_gelu operation\")\n",
    "    raise\n",
    "\n",
    "class MaxNewGELUActivation(nn.Module):\n",
    "    \"\"\"MAX implementation of NewGELUActivation using our custom operation\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        output = x.new_empty(x.shape)\n",
    "\n",
    "        # Use our custom NewGELU operation\n",
    "        # The custom op modifies output in-place (destination-passing style)\n",
    "        torch.compile(new_gelu_op)(output, x)\n",
    "\n",
    "        return output\n",
    "\n",
    "def replace_new_gelu_with_max(model):\n",
    "    \"\"\"Replace NewGELUActivation with MAX implementation\"\"\"\n",
    "    replaced_count = 0\n",
    "\n",
    "    def replace_in_module(module):\n",
    "        nonlocal replaced_count\n",
    "        for name, child in module.named_children():\n",
    "            if hasattr(child, '__class__') and 'NewGELUActivation' in child.__class__.__name__:\n",
    "                print(f\"  Replacing NewGELUActivation in {name}\")\n",
    "                setattr(module, name, MaxNewGELUActivation())\n",
    "                replaced_count += 1\n",
    "            else:\n",
    "                replace_in_module(child)\n",
    "\n",
    "    replace_in_module(model)\n",
    "    model.to(device)\n",
    "    return replaced_count\n",
    "\n",
    "gelu_replaced_count = replace_new_gelu_with_max(model)\n",
    "print(f\"Successfully replaced {gelu_replaced_count} NewGELUActivation operations with MAX\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "26ce51d8-0db2-41b3-8712-e5eaf0b79d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAX LayerNorm + MAX NewGELU Results:\n",
      "Turn 1\n",
      "User: Hello, how are you?\n",
      "Bot: I'm good, thanks. How about you?\n",
      "================================================================================\n",
      "Turn 2\n",
      "User: What's your favorite programming language?\n",
      "Bot: Java, Python, C, and C.\n",
      "================================================================================\n",
      "Turn 3\n",
      "User: Tell me about artificial intelligence\n",
      "Bot: It's a thing. It's called AI. I don't know what it is, but it's something. And it can be used to make things. Like computers. Or robots. Whatever. You want it to be. :D\n",
      "================================================================================\n",
      "Turn 4\n",
      "User: What's the meaning of life?\n",
      "Bot: Life is a computer program that makes things\n",
      "================================================================================\n",
      "Turn 5\n",
      "User: Tell me a joke\n",
      "Bot: A joke!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "bot_max_newgelu = DialogueBot(model, tokenizer)\n",
    "\n",
    "print(\"MAX LayerNorm + MAX NewGELU Results:\")\n",
    "max_full_responses = []\n",
    "for i, user_input in enumerate(test_inputs, 1):\n",
    "    response = bot_max_newgelu.generate_response(user_input)\n",
    "    max_full_responses.append(response)\n",
    "    print(f\"Turn {i}\")\n",
    "    print(f\"User: {user_input}\")\n",
    "    print(f\"Bot: {response}\")\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c21a98-6dc1-42cd-bf04-f2c8600df495",
   "metadata": {},
   "source": [
    "## Surgically replace the whole `GPT2MLP` layer with MAX\n",
    "\n",
    "Next, we implement the `GPT2MLP` layer in MAX and replace all the `GPT2MLP` layers from PyTorch model and get benefit of MAX Graph Compiler Optimizations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "54184f0e-ef7d-4892-8068-191d012ea5ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 1024)\n",
       "    (wpe): Embedding(1024, 1024)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-23): 24 x GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=3072, nx=1024)\n",
       "          (c_proj): Conv1D(nf=1024, nx=1024)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=4096, nx=1024)\n",
       "          (c_proj): Conv1D(nf=1024, nx=4096)\n",
       "          (act): MaxNewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "814ba45a-11d8-48e3-874c-a05490a4645c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found GPT2MLP: mlp\n",
      "c_fc shape: torch.Size([1024, 4096]) (nx=1024, nf=4096)\n",
      "c_proj shape: torch.Size([4096, 1024]) (nx=4096, nf=1024)\n",
      "-> Replaced with MAX implementation\n",
      "Found GPT2MLP: mlp\n",
      "c_fc shape: torch.Size([1024, 4096]) (nx=1024, nf=4096)\n",
      "c_proj shape: torch.Size([4096, 1024]) (nx=4096, nf=1024)\n",
      "-> Replaced with MAX implementation\n",
      "Found GPT2MLP: mlp\n",
      "c_fc shape: torch.Size([1024, 4096]) (nx=1024, nf=4096)\n",
      "c_proj shape: torch.Size([4096, 1024]) (nx=4096, nf=1024)\n",
      "-> Replaced with MAX implementation\n",
      "Found GPT2MLP: mlp\n",
      "c_fc shape: torch.Size([1024, 4096]) (nx=1024, nf=4096)\n",
      "c_proj shape: torch.Size([4096, 1024]) (nx=4096, nf=1024)\n",
      "-> Replaced with MAX implementation\n",
      "Found GPT2MLP: mlp\n",
      "c_fc shape: torch.Size([1024, 4096]) (nx=1024, nf=4096)\n",
      "c_proj shape: torch.Size([4096, 1024]) (nx=4096, nf=1024)\n",
      "-> Replaced with MAX implementation\n",
      "Found GPT2MLP: mlp\n",
      "c_fc shape: torch.Size([1024, 4096]) (nx=1024, nf=4096)\n",
      "c_proj shape: torch.Size([4096, 1024]) (nx=4096, nf=1024)\n",
      "-> Replaced with MAX implementation\n",
      "Found GPT2MLP: mlp\n",
      "c_fc shape: torch.Size([1024, 4096]) (nx=1024, nf=4096)\n",
      "c_proj shape: torch.Size([4096, 1024]) (nx=4096, nf=1024)\n",
      "-> Replaced with MAX implementation\n",
      "Found GPT2MLP: mlp\n",
      "c_fc shape: torch.Size([1024, 4096]) (nx=1024, nf=4096)\n",
      "c_proj shape: torch.Size([4096, 1024]) (nx=4096, nf=1024)\n",
      "-> Replaced with MAX implementation\n",
      "Found GPT2MLP: mlp\n",
      "c_fc shape: torch.Size([1024, 4096]) (nx=1024, nf=4096)\n",
      "c_proj shape: torch.Size([4096, 1024]) (nx=4096, nf=1024)\n",
      "-> Replaced with MAX implementation\n",
      "Found GPT2MLP: mlp\n",
      "c_fc shape: torch.Size([1024, 4096]) (nx=1024, nf=4096)\n",
      "c_proj shape: torch.Size([4096, 1024]) (nx=4096, nf=1024)\n",
      "-> Replaced with MAX implementation\n",
      "Found GPT2MLP: mlp\n",
      "c_fc shape: torch.Size([1024, 4096]) (nx=1024, nf=4096)\n",
      "c_proj shape: torch.Size([4096, 1024]) (nx=4096, nf=1024)\n",
      "-> Replaced with MAX implementation\n",
      "Found GPT2MLP: mlp\n",
      "c_fc shape: torch.Size([1024, 4096]) (nx=1024, nf=4096)\n",
      "c_proj shape: torch.Size([4096, 1024]) (nx=4096, nf=1024)\n",
      "-> Replaced with MAX implementation\n",
      "Found GPT2MLP: mlp\n",
      "c_fc shape: torch.Size([1024, 4096]) (nx=1024, nf=4096)\n",
      "c_proj shape: torch.Size([4096, 1024]) (nx=4096, nf=1024)\n",
      "-> Replaced with MAX implementation\n",
      "Found GPT2MLP: mlp\n",
      "c_fc shape: torch.Size([1024, 4096]) (nx=1024, nf=4096)\n",
      "c_proj shape: torch.Size([4096, 1024]) (nx=4096, nf=1024)\n",
      "-> Replaced with MAX implementation\n",
      "Found GPT2MLP: mlp\n",
      "c_fc shape: torch.Size([1024, 4096]) (nx=1024, nf=4096)\n",
      "c_proj shape: torch.Size([4096, 1024]) (nx=4096, nf=1024)\n",
      "-> Replaced with MAX implementation\n",
      "Found GPT2MLP: mlp\n",
      "c_fc shape: torch.Size([1024, 4096]) (nx=1024, nf=4096)\n",
      "c_proj shape: torch.Size([4096, 1024]) (nx=4096, nf=1024)\n",
      "-> Replaced with MAX implementation\n",
      "Found GPT2MLP: mlp\n",
      "c_fc shape: torch.Size([1024, 4096]) (nx=1024, nf=4096)\n",
      "c_proj shape: torch.Size([4096, 1024]) (nx=4096, nf=1024)\n",
      "-> Replaced with MAX implementation\n",
      "Found GPT2MLP: mlp\n",
      "c_fc shape: torch.Size([1024, 4096]) (nx=1024, nf=4096)\n",
      "c_proj shape: torch.Size([4096, 1024]) (nx=4096, nf=1024)\n",
      "-> Replaced with MAX implementation\n",
      "Found GPT2MLP: mlp\n",
      "c_fc shape: torch.Size([1024, 4096]) (nx=1024, nf=4096)\n",
      "c_proj shape: torch.Size([4096, 1024]) (nx=4096, nf=1024)\n",
      "-> Replaced with MAX implementation\n",
      "Found GPT2MLP: mlp\n",
      "c_fc shape: torch.Size([1024, 4096]) (nx=1024, nf=4096)\n",
      "c_proj shape: torch.Size([4096, 1024]) (nx=4096, nf=1024)\n",
      "-> Replaced with MAX implementation\n",
      "Found GPT2MLP: mlp\n",
      "c_fc shape: torch.Size([1024, 4096]) (nx=1024, nf=4096)\n",
      "c_proj shape: torch.Size([4096, 1024]) (nx=4096, nf=1024)\n",
      "-> Replaced with MAX implementation\n",
      "Found GPT2MLP: mlp\n",
      "c_fc shape: torch.Size([1024, 4096]) (nx=1024, nf=4096)\n",
      "c_proj shape: torch.Size([4096, 1024]) (nx=4096, nf=1024)\n",
      "-> Replaced with MAX implementation\n",
      "Found GPT2MLP: mlp\n",
      "c_fc shape: torch.Size([1024, 4096]) (nx=1024, nf=4096)\n",
      "c_proj shape: torch.Size([4096, 1024]) (nx=4096, nf=1024)\n",
      "-> Replaced with MAX implementation\n",
      "Found GPT2MLP: mlp\n",
      "c_fc shape: torch.Size([1024, 4096]) (nx=1024, nf=4096)\n",
      "c_proj shape: torch.Size([4096, 1024]) (nx=4096, nf=1024)\n",
      "-> Replaced with MAX implementation\n",
      "Replaced 24 GPT2MPL layers\n"
     ]
    }
   ],
   "source": [
    "import max.torch\n",
    "import torch.nn as nn\n",
    "from max.graph import TensorValue, ops\n",
    "\n",
    "@max.torch.graph_op\n",
    "def max_gpt2_mlp(\n",
    "    x: TensorValue,\n",
    "    c_fc_weight: TensorValue,\n",
    "    c_fc_bias: TensorValue,\n",
    "    c_proj_weight: TensorValue,\n",
    "    c_proj_bias: TensorValue,\n",
    "):\n",
    "    \"\"\"\n",
    "    GPT2MLP using MAX graph operations to replicate HuggingFace Conv1D behavior\n",
    "    Architecture: Conv1D(nx=1024, nf=4096) -> NewGELU -> Conv1D(nx=4096, nf=1024) -> Dropout\n",
    "    Note: HuggingFace Conv1D is actually a linear layer with transposed weights\n",
    "    \"\"\"\n",
    "    # c_fc: Conv1D expansion (nx=1024 -> nf=4096)\n",
    "    expanded = x @ c_fc_weight + c_fc_bias\n",
    "\n",
    "    # NewGELU activation (using built-in GELU with tanh approximation)\n",
    "    gelu_output = ops.gelu(expanded, approximate=\"tanh\")\n",
    "\n",
    "    # c_proj: Conv1D projection (nx=4096 -> nf=1024)\n",
    "    return gelu_output @ c_proj_weight + c_proj_bias\n",
    "\n",
    "\n",
    "class MaxGPT2MLP(nn.Module):\n",
    "    def __init__(self, c_fc, c_proj):\n",
    "        super().__init__()\n",
    "\n",
    "        # Copy weights from original Conv1D layers\n",
    "        # HuggingFace Conv1D weight shape: (nx, nf) - already correct for matmul\n",
    "        # For inference, use regular tensors (no gradients needed)\n",
    "\n",
    "        # c_fc: Conv1D(nx=1024, nf=4096) - expand hidden size\n",
    "        # No transpose needed! HuggingFace Conv1D weights are already in matmul-ready shape\n",
    "        self.c_fc_weight = c_fc.weight.clone()  # (1024, 4096)\n",
    "        self.c_fc_bias = c_fc.bias.clone()\n",
    "\n",
    "        # c_proj: Conv1D(nx=4096, nf=1024) - project back to hidden size\n",
    "        self.c_proj_weight = c_proj.weight.clone()  # (4096, 1024)\n",
    "        self.c_proj_bias = c_proj.bias.clone()\n",
    "\n",
    "        # No dropout, we're only doing inference here\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Create output tensor with same shape as input (1024 -> 4096 -> 1024)\n",
    "        # Input shape: (batch_size, seq_len, 1024)\n",
    "        # Output shape: (batch_size, seq_len, 1024)\n",
    "        output = torch.empty_like(x)\n",
    "\n",
    "        # Use MAX graph operation with destination-passing style\n",
    "        torch.compile(max_gpt2_mlp)(\n",
    "            output,\n",
    "            x,\n",
    "            self.c_fc_weight,\n",
    "            self.c_fc_bias,\n",
    "            self.c_proj_weight,\n",
    "            self.c_proj_bias,\n",
    "        )\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "def replace_gpt2_mlp_with_max(model):\n",
    "    mlp_replaced_count = 0\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _replace_gpt2_mlp_with_max(parent):\n",
    "        \"\"\"Replace GPT2MLP layers with MAX Linear implementation that replicates Conv1D behavior\"\"\"\n",
    "        nonlocal mlp_replaced_count\n",
    "        for name, module in parent.named_children():\n",
    "            if (\n",
    "                \"mlp\" in name.lower()\n",
    "                and hasattr(module, \"c_fc\")\n",
    "                and hasattr(module, \"c_proj\")\n",
    "            ):\n",
    "                print(f\"Found GPT2MLP: {name}\")\n",
    "                print(\n",
    "                    f\"c_fc shape: {module.c_fc.weight.shape} (nx={module.c_fc.weight.shape[0]}, nf={module.c_fc.weight.shape[1]})\"\n",
    "                )\n",
    "                print(\n",
    "                    f\"c_proj shape: {module.c_proj.weight.shape} (nx={module.c_proj.weight.shape[0]}, nf={module.c_proj.weight.shape[1]})\"\n",
    "                )\n",
    "    \n",
    "                max_mlp = MaxGPT2MLP(module.c_fc, module.c_proj)\n",
    "                setattr(parent, name, max_mlp)\n",
    "                mlp_replaced_count += 1\n",
    "                print(\"-> Replaced with MAX implementation\")\n",
    "    \n",
    "    model.apply(_replace_gpt2_mlp_with_max)\n",
    "    model.to(device)\n",
    "    return mlp_replaced_count\n",
    "\n",
    "mlp_replaced_count = replace_gpt2_mlp_with_max(model)\n",
    "print(f\"Replaced {mlp_replaced_count} GPT2MPL layers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2eb4a5b5-3a6d-4c55-b36e-12dc12a7c972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAX LayerNorm + MAX NewGELU + New GPT2MLP Results:\n",
      "Turn 1\n",
      "User: Hello, how are you?\n",
      "Bot: I'm good, thanks. How about you?\n",
      "================================================================================\n",
      "Turn 2\n",
      "User: What's your favorite programming language?\n",
      "Bot: Java, Python, C, and C.\n",
      "================================================================================\n",
      "Turn 3\n",
      "User: Tell me about artificial intelligence\n",
      "Bot: It's a thing. It's called AI. I don't know what it is, but it's something. And it can be used to make things. Like computers. Or robots. Whatever. You want it to be. :D\n",
      "================================================================================\n",
      "Turn 4\n",
      "User: What's the meaning of life?\n",
      "Bot: Life is a computer program that makes things\n",
      "================================================================================\n",
      "Turn 5\n",
      "User: Tell me a joke\n",
      "Bot: A joke!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "bot_max_full = DialogueBot(model, tokenizer)\n",
    "\n",
    "print(\"MAX LayerNorm + MAX NewGELU + New GPT2MLP Results:\")\n",
    "max_full_responses = []\n",
    "for i, user_input in enumerate(test_inputs, 1):\n",
    "    response = bot_max_full.generate_response(user_input)\n",
    "    max_full_responses.append(response)\n",
    "    print(f\"Turn {i}\")\n",
    "    print(f\"User: {user_input}\")\n",
    "    print(f\"Bot: {response}\")\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f7b5f88c-a3f6-4085-81ce-59c6cf65dcfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1: Original PyTorch model\n",
      "Stage 2: Replaced 49 LayerNorm operations with MAX\n",
      "Stage 3: Replaced additional 24 NewGELUActivation operations with MAX\n",
      "Stage 4: Replaced additional 24 with custom GPT2MLP layer\n",
      "Total replacements with MAX: 97\n"
     ]
    }
   ],
   "source": [
    "print(\"Stage 1: Original PyTorch model\")\n",
    "print(f\"Stage 2: Replaced {replaced_count} LayerNorm operations with MAX\")\n",
    "print(f\"Stage 3: Replaced additional {gelu_replaced_count} NewGELUActivation operations with MAX\")\n",
    "print(f\"Stage 4: Replaced additional {mlp_replaced_count} with custom GPT2MLP layer\")\n",
    "print(f\"Total replacements with MAX: {replaced_count + gelu_replaced_count + mlp_replaced_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4b9186-e82e-4565-b712-0f2b9fa74fca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
